#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¶…å‚æ•°è°ƒä¼˜æ¨¡å—

ä½¿ç”¨ç½‘æ ¼æœç´¢ã€éšæœºæœç´¢å’Œè´å¶æ–¯ä¼˜åŒ–è¿›è¡Œè¶…å‚æ•°è°ƒä¼˜
"""

import os
import json
import random
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Any, Optional
import torch
import torch.nn as nn
from sklearn.model_selection import ParameterGrid
from itertools import product
import time
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns

# å¯¼å…¥é¡¹ç›®æ¨¡å—
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config.config import Config
from src.data_preprocessing import DataLoader
from src.model import ModelFactory
from src.trainer import SentimentTrainer

class HyperparameterTuner:
    """è¶…å‚æ•°è°ƒä¼˜å™¨"""
    
    def __init__(self, config: Config, results_dir: str = "results/tuning"):
        self.config = config
        self.results_dir = results_dir
        os.makedirs(results_dir, exist_ok=True)
        
        self.tuning_results = []
        self.best_params = None
        self.best_score = 0.0
        
    def define_search_space(self, model_type: str = 'bert') -> Dict[str, List]:
        """å®šä¹‰æœç´¢ç©ºé—´"""
        if model_type == 'bert':
            search_space = {
                'learning_rate': [1e-5, 2e-5, 3e-5, 5e-5],
                'batch_size': [8, 16, 32],
                'num_epochs': [3, 5, 8],
                'max_length': [64, 128, 256],
                'dropout_rate': [0.1, 0.2, 0.3],
                'weight_decay': [0.01, 0.1, 0.2],
                'warmup_steps': [100, 200, 500],
                'use_focal_loss': [True, False],
                'freeze_bert_layers': [0, 2, 4]
            }
        elif model_type == 'bilstm':
            search_space = {
                'learning_rate': [1e-4, 5e-4, 1e-3, 2e-3],
                'batch_size': [16, 32, 64],
                'num_epochs': [10, 15, 20],
                'max_length': [64, 128, 256],
                'hidden_size': [128, 256, 512],
                'num_layers': [1, 2, 3],
                'dropout_rate': [0.2, 0.3, 0.5],
                'bidirectional': [True],
                'use_attention': [True, False],
                'use_focal_loss': [True, False]
            }
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")
        
        return search_space
    
    def grid_search(self, model_type: str = 'bert', max_trials: int = 50) -> Dict[str, Any]:
        """ç½‘æ ¼æœç´¢"""
        print(f"å¼€å§‹ç½‘æ ¼æœç´¢ - æ¨¡å‹ç±»å‹: {model_type}")
        
        search_space = self.define_search_space(model_type)
        
        # ç”Ÿæˆæ‰€æœ‰å‚æ•°ç»„åˆ
        param_grid = list(ParameterGrid(search_space))
        
        # å¦‚æœç»„åˆå¤ªå¤šï¼Œéšæœºé€‰æ‹©ä¸€éƒ¨åˆ†
        if len(param_grid) > max_trials:
            param_grid = random.sample(param_grid, max_trials)
            print(f"å‚æ•°ç»„åˆè¿‡å¤šï¼Œéšæœºé€‰æ‹© {max_trials} ä¸ªç»„åˆè¿›è¡Œæœç´¢")
        
        print(f"æ€»å…±éœ€è¦æµ‹è¯• {len(param_grid)} ä¸ªå‚æ•°ç»„åˆ")
        
        best_score = 0.0
        best_params = None
        
        for i, params in enumerate(param_grid, 1):
            print(f"\n=== è¯•éªŒ {i}/{len(param_grid)} ===")
            print(f"å‚æ•°: {params}")
            
            try:
                score = self._evaluate_params(params, model_type)
                
                # è®°å½•ç»“æœ
                result = {
                    'trial': i,
                    'params': params,
                    'score': score,
                    'model_type': model_type,
                    'timestamp': datetime.now().isoformat()
                }
                self.tuning_results.append(result)
                
                if score > best_score:
                    best_score = score
                    best_params = params
                    print(f"ğŸ‰ å‘ç°æ›´å¥½çš„å‚æ•°! F1åˆ†æ•°: {score:.4f}")
                
                print(f"å½“å‰F1åˆ†æ•°: {score:.4f}, æœ€ä½³F1åˆ†æ•°: {best_score:.4f}")
                
            except Exception as e:
                print(f"âŒ è¯•éªŒå¤±è´¥: {e}")
                continue
        
        self.best_params = best_params
        self.best_score = best_score
        
        print(f"\n=== ç½‘æ ¼æœç´¢å®Œæˆ ===")
        print(f"æœ€ä½³F1åˆ†æ•°: {best_score:.4f}")
        print(f"æœ€ä½³å‚æ•°: {best_params}")
        
        return {'best_params': best_params, 'best_score': best_score}
    
    def random_search(self, model_type: str = 'bert', n_trials: int = 30) -> Dict[str, Any]:
        """éšæœºæœç´¢"""
        print(f"å¼€å§‹éšæœºæœç´¢ - æ¨¡å‹ç±»å‹: {model_type}, è¯•éªŒæ¬¡æ•°: {n_trials}")
        
        search_space = self.define_search_space(model_type)
        
        best_score = 0.0
        best_params = None
        
        for i in range(1, n_trials + 1):
            print(f"\n=== éšæœºè¯•éªŒ {i}/{n_trials} ===")
            
            # éšæœºé‡‡æ ·å‚æ•°
            params = {}
            for param_name, param_values in search_space.items():
                params[param_name] = random.choice(param_values)
            
            print(f"å‚æ•°: {params}")
            
            try:
                score = self._evaluate_params(params, model_type)
                
                # è®°å½•ç»“æœ
                result = {
                    'trial': i,
                    'params': params,
                    'score': score,
                    'model_type': model_type,
                    'search_type': 'random',
                    'timestamp': datetime.now().isoformat()
                }
                self.tuning_results.append(result)
                
                if score > best_score:
                    best_score = score
                    best_params = params
                    print(f"ğŸ‰ å‘ç°æ›´å¥½çš„å‚æ•°! F1åˆ†æ•°: {score:.4f}")
                
                print(f"å½“å‰F1åˆ†æ•°: {score:.4f}, æœ€ä½³F1åˆ†æ•°: {best_score:.4f}")
                
            except Exception as e:
                print(f"âŒ è¯•éªŒå¤±è´¥: {e}")
                continue
        
        self.best_params = best_params
        self.best_score = best_score
        
        print(f"\n=== éšæœºæœç´¢å®Œæˆ ===")
        print(f"æœ€ä½³F1åˆ†æ•°: {best_score:.4f}")
        print(f"æœ€ä½³å‚æ•°: {best_params}")
        
        return {'best_params': best_params, 'best_score': best_score}
    
    def bayesian_optimization(self, model_type: str = 'bert', n_trials: int = 20) -> Dict[str, Any]:
        """è´å¶æ–¯ä¼˜åŒ–ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        print(f"å¼€å§‹è´å¶æ–¯ä¼˜åŒ– - æ¨¡å‹ç±»å‹: {model_type}, è¯•éªŒæ¬¡æ•°: {n_trials}")
        
        # è¿™é‡Œå®ç°ä¸€ä¸ªç®€åŒ–çš„è´å¶æ–¯ä¼˜åŒ–
        # å®é™…é¡¹ç›®ä¸­å¯ä»¥ä½¿ç”¨ optuna æˆ– hyperopt åº“
        
        search_space = self.define_search_space(model_type)
        
        # åˆå§‹éšæœºè¯•éªŒ
        n_random = min(5, n_trials // 2)
        print(f"è¿›è¡Œ {n_random} æ¬¡éšæœºåˆå§‹åŒ–è¯•éªŒ")
        
        best_score = 0.0
        best_params = None
        
        # éšæœºåˆå§‹åŒ–
        for i in range(1, n_random + 1):
            print(f"\n=== åˆå§‹åŒ–è¯•éªŒ {i}/{n_random} ===")
            
            params = {}
            for param_name, param_values in search_space.items():
                params[param_name] = random.choice(param_values)
            
            print(f"å‚æ•°: {params}")
            
            try:
                score = self._evaluate_params(params, model_type)
                
                result = {
                    'trial': i,
                    'params': params,
                    'score': score,
                    'model_type': model_type,
                    'search_type': 'bayesian_init',
                    'timestamp': datetime.now().isoformat()
                }
                self.tuning_results.append(result)
                
                if score > best_score:
                    best_score = score
                    best_params = params
                    print(f"ğŸ‰ å‘ç°æ›´å¥½çš„å‚æ•°! F1åˆ†æ•°: {score:.4f}")
                
            except Exception as e:
                print(f"âŒ è¯•éªŒå¤±è´¥: {e}")
                continue
        
        # åŸºäºå†å²ç»“æœçš„æ™ºèƒ½æœç´¢
        for i in range(n_random + 1, n_trials + 1):
            print(f"\n=== è´å¶æ–¯è¯•éªŒ {i}/{n_trials} ===")
            
            # åŸºäºå†å²æœ€ä½³ç»“æœç”Ÿæˆæ–°å‚æ•°
            params = self._generate_smart_params(search_space, best_params)
            print(f"å‚æ•°: {params}")
            
            try:
                score = self._evaluate_params(params, model_type)
                
                result = {
                    'trial': i,
                    'params': params,
                    'score': score,
                    'model_type': model_type,
                    'search_type': 'bayesian',
                    'timestamp': datetime.now().isoformat()
                }
                self.tuning_results.append(result)
                
                if score > best_score:
                    best_score = score
                    best_params = params
                    print(f"ğŸ‰ å‘ç°æ›´å¥½çš„å‚æ•°! F1åˆ†æ•°: {score:.4f}")
                
            except Exception as e:
                print(f"âŒ è¯•éªŒå¤±è´¥: {e}")
                continue
        
        self.best_params = best_params
        self.best_score = best_score
        
        print(f"\n=== è´å¶æ–¯ä¼˜åŒ–å®Œæˆ ===")
        print(f"æœ€ä½³F1åˆ†æ•°: {best_score:.4f}")
        print(f"æœ€ä½³å‚æ•°: {best_params}")
        
        return {'best_params': best_params, 'best_score': best_score}
    
    def _generate_smart_params(self, search_space: Dict[str, List], 
                              best_params: Dict[str, Any]) -> Dict[str, Any]:
        """åŸºäºæœ€ä½³å‚æ•°ç”Ÿæˆæ–°çš„å‚æ•°ç»„åˆ"""
        if best_params is None:
            # å¦‚æœæ²¡æœ‰æœ€ä½³å‚æ•°ï¼Œéšæœºç”Ÿæˆ
            params = {}
            for param_name, param_values in search_space.items():
                params[param_name] = random.choice(param_values)
            return params
        
        params = best_params.copy()
        
        # éšæœºæ‰°åŠ¨ä¸€äº›å‚æ•°
        n_perturb = random.randint(1, min(3, len(search_space)))
        params_to_perturb = random.sample(list(search_space.keys()), n_perturb)
        
        for param_name in params_to_perturb:
            param_values = search_space[param_name]
            current_value = params[param_name]
            
            if isinstance(current_value, (int, float)):
                # æ•°å€¼å‚æ•°ï¼šåœ¨é‚»è¿‘å€¼ä¸­é€‰æ‹©
                try:
                    current_idx = param_values.index(current_value)
                    # é€‰æ‹©é‚»è¿‘çš„å€¼
                    candidates = []
                    if current_idx > 0:
                        candidates.append(param_values[current_idx - 1])
                    if current_idx < len(param_values) - 1:
                        candidates.append(param_values[current_idx + 1])
                    
                    if candidates:
                        params[param_name] = random.choice(candidates)
                    else:
                        params[param_name] = random.choice(param_values)
                except ValueError:
                    params[param_name] = random.choice(param_values)
            else:
                # å…¶ä»–ç±»å‹å‚æ•°ï¼šéšæœºé€‰æ‹©
                params[param_name] = random.choice(param_values)
        
        return params
    
    def _evaluate_params(self, params: Dict[str, Any], model_type: str) -> float:
        """è¯„ä¼°å‚æ•°ç»„åˆ"""
        # æ›´æ–°é…ç½®
        config = self.config.copy()
        
        # æ›´æ–°æ¨¡å‹å‚æ•°
        config.MODEL_TYPE = model_type
        for key, value in params.items():
            if hasattr(config, key.upper()):
                setattr(config, key.upper(), value)
            elif key == 'learning_rate':
                config.LEARNING_RATE = value
            elif key == 'batch_size':
                config.BATCH_SIZE = value
            elif key == 'num_epochs':
                config.NUM_EPOCHS = value
            elif key == 'max_length':
                config.MAX_LENGTH = value
            elif key == 'dropout_rate':
                config.DROPOUT_RATE = value
            elif key == 'weight_decay':
                config.WEIGHT_DECAY = value
            elif key == 'warmup_steps':
                config.WARMUP_STEPS = value
            elif key == 'use_focal_loss':
                config.USE_FOCAL_LOSS = value
            elif key == 'freeze_bert_layers':
                config.FREEZE_BERT_LAYERS = value
            elif key == 'hidden_size':
                config.HIDDEN_SIZE = value
            elif key == 'num_layers':
                config.NUM_LAYERS = value
            elif key == 'bidirectional':
                config.BIDIRECTIONAL = value
            elif key == 'use_attention':
                config.USE_ATTENTION = value
        
        # è°ƒæ•´æ‰¹æ¬¡å¤§å°ä»¥é€‚åº”GPUå†…å­˜
        if not torch.cuda.is_available() and config.BATCH_SIZE > 16:
            config.BATCH_SIZE = 16
        
        try:
            # å‡†å¤‡æ•°æ®
            data_loader = DataLoader(config)
            train_loader, val_loader, test_loader = data_loader.get_data_loaders()
            
            # åˆ›å»ºæ¨¡å‹
            model = ModelFactory.create_model(config.MODEL_TYPE, config)
            
            # è®­ç»ƒæ¨¡å‹
            trainer = SentimentTrainer(model, config)
            
            # ä½¿ç”¨è¾ƒå°‘çš„epochè¿›è¡Œå¿«é€Ÿè¯„ä¼°
            original_epochs = config.NUM_EPOCHS
            config.NUM_EPOCHS = min(3, config.NUM_EPOCHS)  # æœ€å¤š3ä¸ªepoch
            
            history = trainer.train(train_loader, val_loader)
            
            # æ¢å¤åŸå§‹epochæ•°
            config.NUM_EPOCHS = original_epochs
            
            # è¿”å›æœ€ä½³éªŒè¯F1åˆ†æ•°
            best_f1 = max(history['val_f1']) if history['val_f1'] else 0.0
            
            # æ¸…ç†GPUå†…å­˜
            del model, trainer
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            return best_f1
            
        except Exception as e:
            print(f"è¯„ä¼°å‚æ•°æ—¶å‡ºé”™: {e}")
            # æ¸…ç†GPUå†…å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            return 0.0
    
    def save_results(self, filename: str = None):
        """ä¿å­˜è°ƒä¼˜ç»“æœ"""
        if filename is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"tuning_results_{timestamp}.json"
        
        filepath = os.path.join(self.results_dir, filename)
        
        results = {
            'best_params': self.best_params,
            'best_score': self.best_score,
            'all_results': self.tuning_results,
            'summary': {
                'total_trials': len(self.tuning_results),
                'best_trial': max(self.tuning_results, key=lambda x: x['score']) if self.tuning_results else None
            }
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"è°ƒä¼˜ç»“æœå·²ä¿å­˜: {filepath}")
    
    def load_results(self, filename: str):
        """åŠ è½½è°ƒä¼˜ç»“æœ"""
        filepath = os.path.join(self.results_dir, filename)
        
        if os.path.exists(filepath):
            with open(filepath, 'r', encoding='utf-8') as f:
                results = json.load(f)
            
            self.best_params = results.get('best_params')
            self.best_score = results.get('best_score', 0.0)
            self.tuning_results = results.get('all_results', [])
            
            print(f"å·²åŠ è½½è°ƒä¼˜ç»“æœ: {filepath}")
            print(f"æœ€ä½³F1åˆ†æ•°: {self.best_score:.4f}")
        else:
            print(f"ç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
    
    def plot_tuning_results(self, save_path: str = None):
        """ç»˜åˆ¶è°ƒä¼˜ç»“æœ"""
        if not self.tuning_results:
            print("æ²¡æœ‰è°ƒä¼˜ç»“æœå¯ç»˜åˆ¶")
            return
        
        # è½¬æ¢ä¸ºDataFrame
        df = pd.DataFrame(self.tuning_results)
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('è¶…å‚æ•°è°ƒä¼˜ç»“æœåˆ†æ', fontsize=16, fontweight='bold')
        
        # 1. åˆ†æ•°éšè¯•éªŒæ¬¡æ•°çš„å˜åŒ–
        axes[0, 0].plot(df['trial'], df['score'], 'b-o', alpha=0.7)
        axes[0, 0].axhline(y=self.best_score, color='r', linestyle='--', 
                          label=f'æœ€ä½³åˆ†æ•°: {self.best_score:.4f}')
        axes[0, 0].set_title('F1åˆ†æ•°éšè¯•éªŒæ¬¡æ•°å˜åŒ–')
        axes[0, 0].set_xlabel('è¯•éªŒæ¬¡æ•°')
        axes[0, 0].set_ylabel('F1åˆ†æ•°')
        axes[0, 0].legend()
        axes[0, 0].grid(True)
        
        # 2. åˆ†æ•°åˆ†å¸ƒç›´æ–¹å›¾
        axes[0, 1].hist(df['score'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')
        axes[0, 1].axvline(x=self.best_score, color='r', linestyle='--', 
                          label=f'æœ€ä½³åˆ†æ•°: {self.best_score:.4f}')
        axes[0, 1].set_title('F1åˆ†æ•°åˆ†å¸ƒ')
        axes[0, 1].set_xlabel('F1åˆ†æ•°')
        axes[0, 1].set_ylabel('é¢‘æ¬¡')
        axes[0, 1].legend()
        
        # 3. å‚æ•°é‡è¦æ€§åˆ†æï¼ˆä»¥å­¦ä¹ ç‡ä¸ºä¾‹ï¼‰
        if 'params' in df.columns:
            learning_rates = []
            scores = []
            
            for _, row in df.iterrows():
                params = row['params']
                if 'learning_rate' in params:
                    learning_rates.append(params['learning_rate'])
                    scores.append(row['score'])
            
            if learning_rates:
                axes[1, 0].scatter(learning_rates, scores, alpha=0.7)
                axes[1, 0].set_title('å­¦ä¹ ç‡ vs F1åˆ†æ•°')
                axes[1, 0].set_xlabel('å­¦ä¹ ç‡')
                axes[1, 0].set_ylabel('F1åˆ†æ•°')
                axes[1, 0].set_xscale('log')
                axes[1, 0].grid(True)
        
        # 4. Top 10 è¯•éªŒç»“æœ
        top_results = df.nlargest(min(10, len(df)), 'score')
        axes[1, 1].barh(range(len(top_results)), top_results['score'])
        axes[1, 1].set_title('Top 10 è¯•éªŒç»“æœ')
        axes[1, 1].set_xlabel('F1åˆ†æ•°')
        axes[1, 1].set_ylabel('è¯•éªŒæ’å')
        axes[1, 1].set_yticks(range(len(top_results)))
        axes[1, 1].set_yticklabels([f'è¯•éªŒ {t}' for t in top_results['trial']])
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"è°ƒä¼˜ç»“æœå›¾å·²ä¿å­˜: {save_path}")
        
        plt.show()
    
    def generate_tuning_report(self, output_file: str = None) -> str:
        """ç”Ÿæˆè°ƒä¼˜æŠ¥å‘Š"""
        if not self.tuning_results:
            return "æ²¡æœ‰è°ƒä¼˜ç»“æœå¯ç”ŸæˆæŠ¥å‘Š"
        
        df = pd.DataFrame(self.tuning_results)
        
        report = []
        report.append("# è¶…å‚æ•°è°ƒä¼˜æŠ¥å‘Š")
        report.append(f"\nç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"\næ€»è¯•éªŒæ¬¡æ•°: {len(self.tuning_results)}")
        
        # æœ€ä½³ç»“æœ
        report.append("\n## ğŸ† æœ€ä½³ç»“æœ")
        report.append(f"- **æœ€ä½³F1åˆ†æ•°**: {self.best_score:.4f}")
        report.append(f"- **æœ€ä½³å‚æ•°**: {json.dumps(self.best_params, indent=2, ensure_ascii=False)}")
        
        # ç»Ÿè®¡ä¿¡æ¯
        report.append("\n## ğŸ“Š ç»Ÿè®¡ä¿¡æ¯")
        report.append(f"- **å¹³å‡F1åˆ†æ•°**: {df['score'].mean():.4f}")
        report.append(f"- **F1åˆ†æ•°æ ‡å‡†å·®**: {df['score'].std():.4f}")
        report.append(f"- **æœ€é«˜F1åˆ†æ•°**: {df['score'].max():.4f}")
        report.append(f"- **æœ€ä½F1åˆ†æ•°**: {df['score'].min():.4f}")
        
        # Top 5 ç»“æœ
        top_5 = df.nlargest(5, 'score')
        report.append("\n## ğŸ¥‡ Top 5 ç»“æœ")
        for i, (_, row) in enumerate(top_5.iterrows(), 1):
            report.append(f"\n### ç¬¬ {i} å")
            report.append(f"- **F1åˆ†æ•°**: {row['score']:.4f}")
            report.append(f"- **è¯•éªŒç¼–å·**: {row['trial']}")
            report.append(f"- **å‚æ•°**: {json.dumps(row['params'], indent=2, ensure_ascii=False)}")
        
        report_text = "\n".join(report)
        
        if output_file:
            filepath = os.path.join(self.results_dir, output_file)
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(report_text)
            print(f"è°ƒä¼˜æŠ¥å‘Šå·²ä¿å­˜: {filepath}")
        
        return report_text

def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºè¶…å‚æ•°è°ƒä¼˜"""
    # åŠ è½½é…ç½®
    config = Config()
    
    # åˆ›å»ºè°ƒä¼˜å™¨
    tuner = HyperparameterTuner(config)
    
    print("=== è¶…å‚æ•°è°ƒä¼˜æ¼”ç¤º ===")
    print("1. éšæœºæœç´¢ (å¿«é€Ÿ)")
    print("2. ç½‘æ ¼æœç´¢ (å…¨é¢)")
    print("3. è´å¶æ–¯ä¼˜åŒ– (æ™ºèƒ½)")
    
    choice = input("è¯·é€‰æ‹©è°ƒä¼˜æ–¹æ³• (1-3): ").strip()
    model_type = input("è¯·é€‰æ‹©æ¨¡å‹ç±»å‹ (bert/bilstm): ").strip().lower()
    
    if model_type not in ['bert', 'bilstm']:
        model_type = 'bert'
    
    start_time = time.time()
    
    if choice == '1':
        result = tuner.random_search(model_type=model_type, n_trials=10)
    elif choice == '2':
        result = tuner.grid_search(model_type=model_type, max_trials=20)
    elif choice == '3':
        result = tuner.bayesian_optimization(model_type=model_type, n_trials=15)
    else:
        print("æ— æ•ˆé€‰æ‹©ï¼Œä½¿ç”¨éšæœºæœç´¢")
        result = tuner.random_search(model_type=model_type, n_trials=10)
    
    end_time = time.time()
    
    print(f"\n=== è°ƒä¼˜å®Œæˆ ===")
    print(f"æ€»è€—æ—¶: {end_time - start_time:.2f}ç§’")
    print(f"æœ€ä½³F1åˆ†æ•°: {result['best_score']:.4f}")
    print(f"æœ€ä½³å‚æ•°: {result['best_params']}")
    
    # ä¿å­˜ç»“æœ
    tuner.save_results()
    
    # ç»˜åˆ¶ç»“æœ
    plot_path = os.path.join(tuner.results_dir, "tuning_analysis.png")
    tuner.plot_tuning_results(plot_path)
    
    # ç”ŸæˆæŠ¥å‘Š
    report_path = "tuning_report.md"
    tuner.generate_tuning_report(report_path)

if __name__ == "__main__":
    import sys
    main()