#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨¡å‹æ¯”è¾ƒå’Œåˆ†ææ¨¡å—

æä¾›å¤šä¸ªæ¨¡å‹çš„è®­ç»ƒã€æ¯”è¾ƒå’Œåˆ†æåŠŸèƒ½
"""

import os
import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Any
import torch
import torch.nn as nn
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import cross_val_score, StratifiedKFold
import time
from datetime import datetime

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class ModelComparator:
    """æ¨¡å‹æ¯”è¾ƒå™¨"""
    
    def __init__(self, results_dir: str = "results/comparison"):
        self.results_dir = results_dir
        os.makedirs(results_dir, exist_ok=True)
        
        self.models_info = {}
        self.comparison_results = {}
        
    def add_model_result(self, model_name: str, model_path: str, 
                        metrics: Dict[str, float], training_time: float,
                        model_config: Dict[str, Any] = None):
        """æ·»åŠ æ¨¡å‹ç»“æœ"""
        self.models_info[model_name] = {
            'model_path': model_path,
            'metrics': metrics,
            'training_time': training_time,
            'config': model_config or {},
            'timestamp': datetime.now().isoformat()
        }
        
        print(f"å·²æ·»åŠ æ¨¡å‹: {model_name}")
        print(f"F1åˆ†æ•°: {metrics.get('f1_score', 'N/A'):.4f}")
        print(f"å‡†ç¡®ç‡: {metrics.get('accuracy', 'N/A'):.4f}")
        print(f"è®­ç»ƒæ—¶é—´: {training_time:.2f}ç§’")
    
    def load_model_results(self, results_file: str):
        """ä»æ–‡ä»¶åŠ è½½æ¨¡å‹ç»“æœ"""
        if os.path.exists(results_file):
            with open(results_file, 'r', encoding='utf-8') as f:
                self.models_info = json.load(f)
            print(f"å·²åŠ è½½ {len(self.models_info)} ä¸ªæ¨¡å‹ç»“æœ")
        else:
            print(f"ç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {results_file}")
    
    def save_model_results(self, results_file: str):
        """ä¿å­˜æ¨¡å‹ç»“æœåˆ°æ–‡ä»¶"""
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(self.models_info, f, indent=2, ensure_ascii=False)
        print(f"æ¨¡å‹ç»“æœå·²ä¿å­˜: {results_file}")
    
    def compare_models(self) -> pd.DataFrame:
        """æ¯”è¾ƒæ‰€æœ‰æ¨¡å‹"""
        if not self.models_info:
            print("æ²¡æœ‰æ¨¡å‹ç»“æœå¯æ¯”è¾ƒ")
            return pd.DataFrame()
        
        comparison_data = []
        
        for model_name, info in self.models_info.items():
            metrics = info['metrics']
            config = info['config']
            
            row = {
                'æ¨¡å‹åç§°': model_name,
                'F1åˆ†æ•°': metrics.get('f1_score', 0),
                'å‡†ç¡®ç‡': metrics.get('accuracy', 0),
                'ç²¾ç¡®ç‡': metrics.get('precision', 0),
                'å¬å›ç‡': metrics.get('recall', 0),
                'è®­ç»ƒæ—¶é—´(ç§’)': info['training_time'],
                'æ¨¡å‹ç±»å‹': config.get('model_type', 'Unknown'),
                'æ‰¹æ¬¡å¤§å°': config.get('batch_size', 'N/A'),
                'å­¦ä¹ ç‡': config.get('learning_rate', 'N/A'),
                'æœ€å¤§é•¿åº¦': config.get('max_length', 'N/A'),
                'ä½¿ç”¨Focal Loss': config.get('use_focal_loss', False),
                'æ—¶é—´æˆ³': info['timestamp']
            }
            comparison_data.append(row)
        
        df = pd.DataFrame(comparison_data)
        
        # æŒ‰F1åˆ†æ•°æ’åº
        df = df.sort_values('F1åˆ†æ•°', ascending=False).reset_index(drop=True)
        
        return df
    
    def plot_model_comparison(self, save_path: str = None):
        """ç»˜åˆ¶æ¨¡å‹æ¯”è¾ƒå›¾"""
        df = self.compare_models()
        
        if df.empty:
            print("æ²¡æœ‰æ•°æ®å¯ç»˜åˆ¶")
            return
        
        # åˆ›å»ºå­å›¾
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ', fontsize=16, fontweight='bold')
        
        # 1. F1åˆ†æ•°æ¯”è¾ƒ
        ax1 = axes[0, 0]
        bars1 = ax1.bar(range(len(df)), df['F1åˆ†æ•°'], color='skyblue', alpha=0.7)
        ax1.set_title('F1åˆ†æ•°æ¯”è¾ƒ')
        ax1.set_xlabel('æ¨¡å‹')
        ax1.set_ylabel('F1åˆ†æ•°')
        ax1.set_xticks(range(len(df)))
        ax1.set_xticklabels(df['æ¨¡å‹åç§°'], rotation=45, ha='right')
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, bar in enumerate(bars1):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{height:.3f}', ha='center', va='bottom')
        
        # 2. å‡†ç¡®ç‡æ¯”è¾ƒ
        ax2 = axes[0, 1]
        bars2 = ax2.bar(range(len(df)), df['å‡†ç¡®ç‡'], color='lightgreen', alpha=0.7)
        ax2.set_title('å‡†ç¡®ç‡æ¯”è¾ƒ')
        ax2.set_xlabel('æ¨¡å‹')
        ax2.set_ylabel('å‡†ç¡®ç‡')
        ax2.set_xticks(range(len(df)))
        ax2.set_xticklabels(df['æ¨¡å‹åç§°'], rotation=45, ha='right')
        
        for i, bar in enumerate(bars2):
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{height:.3f}', ha='center', va='bottom')
        
        # 3. è®­ç»ƒæ—¶é—´æ¯”è¾ƒ
        ax3 = axes[1, 0]
        bars3 = ax3.bar(range(len(df)), df['è®­ç»ƒæ—¶é—´(ç§’)'], color='orange', alpha=0.7)
        ax3.set_title('è®­ç»ƒæ—¶é—´æ¯”è¾ƒ')
        ax3.set_xlabel('æ¨¡å‹')
        ax3.set_ylabel('è®­ç»ƒæ—¶é—´(ç§’)')
        ax3.set_xticks(range(len(df)))
        ax3.set_xticklabels(df['æ¨¡å‹åç§°'], rotation=45, ha='right')
        
        for i, bar in enumerate(bars3):
            height = bar.get_height()
            ax3.text(bar.get_x() + bar.get_width()/2., height + 10,
                    f'{height:.0f}s', ha='center', va='bottom')
        
        # 4. ç»¼åˆæ€§èƒ½é›·è¾¾å›¾
        ax4 = axes[1, 1]
        
        # é€‰æ‹©å‰3ä¸ªæ¨¡å‹è¿›è¡Œé›·è¾¾å›¾æ¯”è¾ƒ
        top_models = df.head(3)
        
        if len(top_models) > 0:
            metrics = ['F1åˆ†æ•°', 'å‡†ç¡®ç‡', 'ç²¾ç¡®ç‡', 'å¬å›ç‡']
            angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()
            angles += angles[:1]  # é—­åˆå›¾å½¢
            
            ax4 = plt.subplot(2, 2, 4, projection='polar')
            
            colors = ['red', 'blue', 'green']
            
            for i, (_, model) in enumerate(top_models.iterrows()):
                if i >= 3:  # æœ€å¤šæ˜¾ç¤º3ä¸ªæ¨¡å‹
                    break
                
                values = [model[metric] for metric in metrics]
                values += values[:1]  # é—­åˆå›¾å½¢
                
                ax4.plot(angles, values, 'o-', linewidth=2, 
                        label=model['æ¨¡å‹åç§°'], color=colors[i])
                ax4.fill(angles, values, alpha=0.25, color=colors[i])
            
            ax4.set_xticks(angles[:-1])
            ax4.set_xticklabels(metrics)
            ax4.set_ylim(0, 1)
            ax4.set_title('Top 3 æ¨¡å‹ç»¼åˆæ€§èƒ½')
            ax4.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"æ¯”è¾ƒå›¾å·²ä¿å­˜: {save_path}")
        
        plt.show()
    
    def plot_training_curves(self, model_name: str, log_file: str = None):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        if log_file and os.path.exists(log_file):
            # ä»æ—¥å¿—æ–‡ä»¶è¯»å–è®­ç»ƒå†å²
            training_history = self.parse_training_log(log_file)
        else:
            print(f"æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨: {log_file}")
            return
        
        if not training_history:
            print("æ²¡æœ‰æ‰¾åˆ°è®­ç»ƒå†å²æ•°æ®")
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle(f'{model_name} è®­ç»ƒæ›²çº¿', fontsize=16, fontweight='bold')
        
        epochs = range(1, len(training_history['train_loss']) + 1)
        
        # æŸå¤±æ›²çº¿
        axes[0, 0].plot(epochs, training_history['train_loss'], 'b-', label='è®­ç»ƒæŸå¤±')
        if 'val_loss' in training_history:
            axes[0, 0].plot(epochs, training_history['val_loss'], 'r-', label='éªŒè¯æŸå¤±')
        axes[0, 0].set_title('æŸå¤±æ›²çº¿')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].legend()
        axes[0, 0].grid(True)
        
        # å‡†ç¡®ç‡æ›²çº¿
        if 'train_acc' in training_history:
            axes[0, 1].plot(epochs, training_history['train_acc'], 'b-', label='è®­ç»ƒå‡†ç¡®ç‡')
        if 'val_acc' in training_history:
            axes[0, 1].plot(epochs, training_history['val_acc'], 'r-', label='éªŒè¯å‡†ç¡®ç‡')
        axes[0, 1].set_title('å‡†ç¡®ç‡æ›²çº¿')
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('Accuracy')
        axes[0, 1].legend()
        axes[0, 1].grid(True)
        
        # F1åˆ†æ•°æ›²çº¿
        if 'train_f1' in training_history:
            axes[1, 0].plot(epochs, training_history['train_f1'], 'b-', label='è®­ç»ƒF1')
        if 'val_f1' in training_history:
            axes[1, 0].plot(epochs, training_history['val_f1'], 'r-', label='éªŒè¯F1')
        axes[1, 0].set_title('F1åˆ†æ•°æ›²çº¿')
        axes[1, 0].set_xlabel('Epoch')
        axes[1, 0].set_ylabel('F1 Score')
        axes[1, 0].legend()
        axes[1, 0].grid(True)
        
        # å­¦ä¹ ç‡æ›²çº¿
        if 'learning_rate' in training_history:
            axes[1, 1].plot(epochs, training_history['learning_rate'], 'g-', label='å­¦ä¹ ç‡')
            axes[1, 1].set_title('å­¦ä¹ ç‡æ›²çº¿')
            axes[1, 1].set_xlabel('Epoch')
            axes[1, 1].set_ylabel('Learning Rate')
            axes[1, 1].legend()
            axes[1, 1].grid(True)
        else:
            axes[1, 1].axis('off')
        
        plt.tight_layout()
        
        save_path = os.path.join(self.results_dir, f'{model_name}_training_curves.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"è®­ç»ƒæ›²çº¿å·²ä¿å­˜: {save_path}")
        
        plt.show()
    
    def parse_training_log(self, log_file: str) -> Dict[str, List[float]]:
        """è§£æè®­ç»ƒæ—¥å¿—æ–‡ä»¶"""
        training_history = {
            'train_loss': [],
            'val_loss': [],
            'train_acc': [],
            'val_acc': [],
            'train_f1': [],
            'val_f1': [],
            'learning_rate': []
        }
        
        try:
            with open(log_file, 'r', encoding='utf-8') as f:
                for line in f:
                    # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„æ—¥å¿—æ ¼å¼æ¥è§£æ
                    # ç¤ºä¾‹æ ¼å¼: "Epoch 1/5 - Train Loss: 0.5, Val Loss: 0.4, Train Acc: 0.8, Val Acc: 0.85"
                    if 'Epoch' in line and 'Train Loss' in line:
                        parts = line.strip().split(' - ')
                        if len(parts) > 1:
                            metrics_part = parts[1]
                            
                            # è§£æå„ç§æŒ‡æ ‡
                            if 'Train Loss:' in metrics_part:
                                train_loss = float(metrics_part.split('Train Loss:')[1].split(',')[0].strip())
                                training_history['train_loss'].append(train_loss)
                            
                            if 'Val Loss:' in metrics_part:
                                val_loss = float(metrics_part.split('Val Loss:')[1].split(',')[0].strip())
                                training_history['val_loss'].append(val_loss)
                            
                            # å¯ä»¥ç»§ç»­æ·»åŠ å…¶ä»–æŒ‡æ ‡çš„è§£æ
        except Exception as e:
            print(f"è§£ææ—¥å¿—æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        
        return training_history
    
    def generate_report(self, output_file: str = None):
        """ç”Ÿæˆè¯¦ç»†çš„æ¯”è¾ƒæŠ¥å‘Š"""
        df = self.compare_models()
        
        if df.empty:
            print("æ²¡æœ‰æ¨¡å‹æ•°æ®å¯ç”ŸæˆæŠ¥å‘Š")
            return
        
        report = []
        report.append("# YouTubeè¯„è®ºæƒ…æ„Ÿåˆ†æ - æ¨¡å‹æ¯”è¾ƒæŠ¥å‘Š")
        report.append(f"\nç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"\næ€»å…±æ¯”è¾ƒäº† {len(df)} ä¸ªæ¨¡å‹\n")
        
        # æœ€ä½³æ¨¡å‹
        best_model = df.iloc[0]
        report.append("## ğŸ† æœ€ä½³æ¨¡å‹")
        report.append(f"- **æ¨¡å‹åç§°**: {best_model['æ¨¡å‹åç§°']}")
        report.append(f"- **F1åˆ†æ•°**: {best_model['F1åˆ†æ•°']:.4f}")
        report.append(f"- **å‡†ç¡®ç‡**: {best_model['å‡†ç¡®ç‡']:.4f}")
        report.append(f"- **ç²¾ç¡®ç‡**: {best_model['ç²¾ç¡®ç‡']:.4f}")
        report.append(f"- **å¬å›ç‡**: {best_model['å¬å›ç‡']:.4f}")
        report.append(f"- **è®­ç»ƒæ—¶é—´**: {best_model['è®­ç»ƒæ—¶é—´(ç§’)']:.2f}ç§’")
        report.append(f"- **æ¨¡å‹ç±»å‹**: {best_model['æ¨¡å‹ç±»å‹']}")
        
        # æ€§èƒ½ç»Ÿè®¡
        report.append("\n## ğŸ“Š æ€§èƒ½ç»Ÿè®¡")
        report.append(f"- **å¹³å‡F1åˆ†æ•°**: {df['F1åˆ†æ•°'].mean():.4f} Â± {df['F1åˆ†æ•°'].std():.4f}")
        report.append(f"- **å¹³å‡å‡†ç¡®ç‡**: {df['å‡†ç¡®ç‡'].mean():.4f} Â± {df['å‡†ç¡®ç‡'].std():.4f}")
        report.append(f"- **å¹³å‡è®­ç»ƒæ—¶é—´**: {df['è®­ç»ƒæ—¶é—´(ç§’)'].mean():.2f} Â± {df['è®­ç»ƒæ—¶é—´(ç§’)'].std():.2f}ç§’")
        
        # è¯¦ç»†æ¯”è¾ƒè¡¨
        report.append("\n## ğŸ“‹ è¯¦ç»†æ¯”è¾ƒ")
        report.append("\n| æ’å | æ¨¡å‹åç§° | F1åˆ†æ•° | å‡†ç¡®ç‡ | è®­ç»ƒæ—¶é—´(ç§’) | æ¨¡å‹ç±»å‹ |")
        report.append("|------|----------|--------|--------|--------------|----------|")
        
        for i, (_, row) in enumerate(df.iterrows(), 1):
            report.append(
                f"| {i} | {row['æ¨¡å‹åç§°']} | {row['F1åˆ†æ•°']:.4f} | "
                f"{row['å‡†ç¡®ç‡']:.4f} | {row['è®­ç»ƒæ—¶é—´(ç§’)']:.2f} | {row['æ¨¡å‹ç±»å‹']} |"
            )
        
        # å»ºè®®
        report.append("\n## ğŸ’¡ å»ºè®®")
        
        if len(df) > 1:
            f1_diff = df.iloc[0]['F1åˆ†æ•°'] - df.iloc[1]['F1åˆ†æ•°']
            if f1_diff > 0.05:
                report.append(f"- æœ€ä½³æ¨¡å‹ '{best_model['æ¨¡å‹åç§°']}' æ˜æ˜¾ä¼˜äºå…¶ä»–æ¨¡å‹")
            else:
                report.append("- å¤šä¸ªæ¨¡å‹æ€§èƒ½ç›¸è¿‘ï¼Œå¯ä»¥è€ƒè™‘é›†æˆå­¦ä¹ ")
        
        if best_model['è®­ç»ƒæ—¶é—´(ç§’)'] > df['è®­ç»ƒæ—¶é—´(ç§’)'].median() * 2:
            report.append("- æœ€ä½³æ¨¡å‹è®­ç»ƒæ—¶é—´è¾ƒé•¿ï¼Œå¯ä»¥è€ƒè™‘æ¨¡å‹å‹ç¼©æˆ–ä¼˜åŒ–")
        
        if best_model['F1åˆ†æ•°'] < 0.85:
            report.append("- F1åˆ†æ•°è¿˜æœ‰æå‡ç©ºé—´ï¼Œå»ºè®®å°è¯•æ•°æ®å¢å¼ºæˆ–è¶…å‚æ•°è°ƒä¼˜")
        
        report_text = "\n".join(report)
        
        if output_file:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(report_text)
            print(f"æ¯”è¾ƒæŠ¥å‘Šå·²ä¿å­˜: {output_file}")
        
        print("\n" + report_text)
        
        return report_text
    
    def recommend_best_model(self) -> Dict[str, Any]:
        """æ¨èæœ€ä½³æ¨¡å‹"""
        df = self.compare_models()
        
        if df.empty:
            return {}
        
        # ç»¼åˆè¯„åˆ†ï¼šF1åˆ†æ•°æƒé‡0.4ï¼Œå‡†ç¡®ç‡æƒé‡0.3ï¼Œè®­ç»ƒæ—¶é—´æƒé‡0.3ï¼ˆè¶ŠçŸ­è¶Šå¥½ï¼‰
        df['ç»¼åˆè¯„åˆ†'] = (
            df['F1åˆ†æ•°'] * 0.4 + 
            df['å‡†ç¡®ç‡'] * 0.3 + 
            (1 - df['è®­ç»ƒæ—¶é—´(ç§’)'] / df['è®­ç»ƒæ—¶é—´(ç§’)'].max()) * 0.3
        )
        
        best_model = df.loc[df['ç»¼åˆè¯„åˆ†'].idxmax()]
        
        recommendation = {
            'model_name': best_model['æ¨¡å‹åç§°'],
            'model_path': self.models_info[best_model['æ¨¡å‹åç§°']]['model_path'],
            'f1_score': best_model['F1åˆ†æ•°'],
            'accuracy': best_model['å‡†ç¡®ç‡'],
            'training_time': best_model['è®­ç»ƒæ—¶é—´(ç§’)'],
            'composite_score': best_model['ç»¼åˆè¯„åˆ†'],
            'config': self.models_info[best_model['æ¨¡å‹åç§°']]['config']
        }
        
        return recommendation

def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºæ¨¡å‹æ¯”è¾ƒåŠŸèƒ½"""
    comparator = ModelComparator()
    
    # ç¤ºä¾‹ï¼šæ·»åŠ ä¸€äº›æ¨¡å‹ç»“æœ
    comparator.add_model_result(
        model_name="BERT-base",
        model_path="models/bert_base_model.pth",
        metrics={
            'f1_score': 0.8756,
            'accuracy': 0.8923,
            'precision': 0.8834,
            'recall': 0.8679
        },
        training_time=1245.6,
        model_config={
            'model_type': 'bert',
            'batch_size': 16,
            'learning_rate': 2e-5,
            'max_length': 128,
            'use_focal_loss': True
        }
    )
    
    comparator.add_model_result(
        model_name="BiLSTM-Attention",
        model_path="models/bilstm_attention_model.pth",
        metrics={
            'f1_score': 0.8234,
            'accuracy': 0.8456,
            'precision': 0.8123,
            'recall': 0.8345
        },
        training_time=856.3,
        model_config={
            'model_type': 'bilstm',
            'batch_size': 32,
            'learning_rate': 1e-3,
            'max_length': 128,
            'use_focal_loss': False
        }
    )
    
    # æ¯”è¾ƒæ¨¡å‹
    print("=== æ¨¡å‹æ¯”è¾ƒ ===")
    comparison_df = comparator.compare_models()
    print(comparison_df)
    
    # ç»˜åˆ¶æ¯”è¾ƒå›¾
    comparator.plot_model_comparison()
    
    # ç”ŸæˆæŠ¥å‘Š
    report_file = os.path.join(comparator.results_dir, "model_comparison_report.md")
    comparator.generate_report(report_file)
    
    # æ¨èæœ€ä½³æ¨¡å‹
    recommendation = comparator.recommend_best_model()
    print(f"\n=== æ¨èæ¨¡å‹ ===")
    print(f"æ¨èæ¨¡å‹: {recommendation.get('model_name', 'N/A')}")
    print(f"ç»¼åˆè¯„åˆ†: {recommendation.get('composite_score', 0):.4f}")

if __name__ == "__main__":
    main()