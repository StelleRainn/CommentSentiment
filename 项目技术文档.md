# YouTubeè¯„è®ºæƒ…æ„Ÿåˆ†æé¡¹ç›®æŠ€æœ¯æ–‡æ¡£

## é¡¹ç›®æ¦‚è¿°

æœ¬é¡¹ç›®æ˜¯ä¸€ä¸ªåŸºäºæ·±åº¦å­¦ä¹ çš„YouTubeè¯„è®ºæƒ…æ„Ÿåˆ†æç³»ç»Ÿï¼Œä¸»è¦ä½¿ç”¨BERTæ¨¡å‹è¿›è¡Œä¸‰åˆ†ç±»æƒ…æ„Ÿåˆ†æï¼ˆæ­£é¢ã€è´Ÿé¢ã€ä¸­æ€§ï¼‰ã€‚é¡¹ç›®åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šé˜¶æ®µä¸€ä½¿ç”¨ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œé˜¶æ®µäºŒé‡‡ç”¨æ·±åº¦å­¦ä¹ æ–¹æ³•ã€‚

---

## 1. é¡¹ç›®æ–‡ä»¶ç»“æ„ä¸ä½œç”¨

### 1.1 æ ¸å¿ƒé…ç½®æ–‡ä»¶

#### `config/config.py`
- **ä½œç”¨**ï¼šé¡¹ç›®çš„æ ¸å¿ƒé…ç½®æ–‡ä»¶
- **ä¸»è¦å†…å®¹**ï¼š
  - æ¨¡å‹å‚æ•°é…ç½®ï¼ˆBERTæ¨¡å‹åç§°ã€æœ€å¤§åºåˆ—é•¿åº¦ã€æ‰¹æ¬¡å¤§å°ç­‰ï¼‰
  - è®­ç»ƒå‚æ•°é…ç½®ï¼ˆå­¦ä¹ ç‡ã€è®­ç»ƒè½®æ•°ã€éªŒè¯é›†æ¯”ä¾‹ç­‰ï¼‰
  - è·¯å¾„é…ç½®ï¼ˆæ•°æ®è·¯å¾„ã€æ¨¡å‹ä¿å­˜è·¯å¾„ç­‰ï¼‰
  - è®¾å¤‡é…ç½®ï¼ˆGPU/CPUé€‰æ‹©ï¼‰

### 1.2 æ•°æ®å¤„ç†æ¨¡å—

#### `src/data_preprocessing.py`
- **ä½œç”¨**ï¼šæ•°æ®é¢„å¤„ç†å’ŒåŠ è½½çš„æ ¸å¿ƒæ¨¡å—
- **ä¸»è¦åŠŸèƒ½**ï¼š
  - `TextPreprocessor`ç±»ï¼šæ–‡æœ¬æ¸…æ´—ã€å»é™¤åœç”¨è¯ã€æ•°æ®å¢å¼º
  - `SentimentDataset`ç±»ï¼šPyTorchæ•°æ®é›†å°è£…
  - `DataLoader`ç±»ï¼šæ•°æ®åŠ è½½ã€é¢„å¤„ç†ã€åˆ†å‰²çš„ä¸»æ§åˆ¶å™¨
  - `load_custom_data()`æ–¹æ³•ï¼šæ”¯æŒè‡ªå®šä¹‰æ•°æ®æ–‡ä»¶åŠ è½½

#### `data_splitter.py`
- **ä½œç”¨**ï¼šç°åœºè€ƒæ ¸ä¸“ç”¨çš„æ•°æ®é›†åˆ’åˆ†å·¥å…·
- **ä¸»è¦åŠŸèƒ½**ï¼š
  - æ™ºèƒ½æ£€æµ‹æ–‡ä»¶æ ¼å¼ï¼ˆCSV/Excelï¼‰
  - è‡ªåŠ¨è¯†åˆ«æ–‡æœ¬åˆ—å’Œæ ‡ç­¾åˆ—
  - åˆ†å±‚åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
  - æ”¯æŒå¤šç§ç¼–ç æ ¼å¼

#### `data_integration.py`
- **ä½œç”¨**ï¼šå¤šæ•°æ®é›†æ•´åˆå·¥å…·
- **ä¸»è¦åŠŸèƒ½**ï¼š
  - åˆå¹¶å¤šä¸ªæ•°æ®æº
  - æ ‡ç­¾æ ‡å‡†åŒ–
  - æ•°æ®å»é‡å’Œæ¸…æ´—

### 1.3 æ¨¡å‹å®šä¹‰æ¨¡å—

#### `src/model.py`
- **ä½œç”¨**ï¼šå®šä¹‰æ·±åº¦å­¦ä¹ æ¨¡å‹æ¶æ„
- **ä¸»è¦å†…å®¹**ï¼š
  - `BertSentimentClassifier`ç±»ï¼šåŸºäºBERTçš„æƒ…æ„Ÿåˆ†ç±»å™¨
  - `create_model()`å‡½æ•°ï¼šæ¨¡å‹åˆ›å»ºå·¥å‚å‡½æ•°
  - æ”¯æŒå±‚å†»ç»“ã€Dropoutç­‰æ­£åˆ™åŒ–æŠ€æœ¯

### 1.4 è®­ç»ƒæ¨¡å—

#### `src/trainer.py`
- **ä½œç”¨**ï¼šæ¨¡å‹è®­ç»ƒçš„æ ¸å¿ƒæ§åˆ¶å™¨
- **ä¸»è¦åŠŸèƒ½**ï¼š
  - è®­ç»ƒå¾ªç¯æ§åˆ¶
  - éªŒè¯å’Œè¯„ä¼°
  - æ¨¡å‹ä¿å­˜å’ŒåŠ è½½
  - æ—©åœæœºåˆ¶
  - æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨ç®¡ç†

#### `train.py`
- **ä½œç”¨**ï¼šæ ‡å‡†è®­ç»ƒè„šæœ¬
- **ç‰¹ç‚¹**ï¼šæ”¯æŒå‘½ä»¤è¡Œå‚æ•°ï¼Œå¯æŒ‡å®šè‡ªå®šä¹‰æ•°æ®è·¯å¾„

#### `train_improved.py`
- **ä½œç”¨**ï¼šæ”¹è¿›çš„è®­ç»ƒè„šæœ¬
- **å¢å¼ºåŠŸèƒ½**ï¼š
  - å­¦ä¹ ç‡è°ƒåº¦
  - æ¢¯åº¦è£å‰ª
  - æ ‡ç­¾å¹³æ»‘
  - æ›´è¯¦ç»†çš„ç›‘æ§å’Œå¯è§†åŒ–
  - æ··åˆç²¾åº¦è®­ç»ƒæ”¯æŒ

### 1.5 è¯„ä¼°å’Œé¢„æµ‹æ¨¡å—

#### `evaluate.py`
- **ä½œç”¨**ï¼šæ¨¡å‹è¯„ä¼°è„šæœ¬
- **åŠŸèƒ½**ï¼šè®¡ç®—å‡†ç¡®ç‡ã€F1åˆ†æ•°ã€æ··æ·†çŸ©é˜µç­‰æŒ‡æ ‡

#### `test.py`
- **ä½œç”¨**ï¼šæ¨¡å‹æµ‹è¯•å’Œé¢„æµ‹è„šæœ¬

### 1.6 è¾…åŠ©å·¥å…·

#### `usage_example.py`
- **ä½œç”¨**ï¼šä½¿ç”¨æŒ‡å—å’Œç¤ºä¾‹
- **å†…å®¹**ï¼šå®Œæ•´çš„ç°åœºè€ƒæ ¸æµç¨‹è¯´æ˜

#### `quick_start.py`
- **ä½œç”¨**ï¼šå¿«é€Ÿå¯åŠ¨è„šæœ¬

---

## 2. è®­ç»ƒæ¨¡å‹çš„æ€è·¯æµç¨‹

### 2.1 æ•´ä½“è®­ç»ƒæµç¨‹

```
æ•°æ®å‡†å¤‡ â†’ æ•°æ®é¢„å¤„ç† â†’ æ¨¡å‹æ„å»º â†’ è®­ç»ƒå¾ªç¯ â†’ æ¨¡å‹è¯„ä¼° â†’ æ¨¡å‹ä¿å­˜
```

### 2.2 è¯¦ç»†æµç¨‹è¯´æ˜

#### æ­¥éª¤1ï¼šæ•°æ®å‡†å¤‡

##### 1.1 æ•°æ®åŠ è½½ç­–ç•¥
```python
# æ”¯æŒå¤šç§æ•°æ®æ ¼å¼
def load_data(file_path):
    if file_path.endswith('.xlsx'):
        data = pd.read_excel(file_path)
    elif file_path.endswith('.csv'):
        # å°è¯•å¤šç§ç¼–ç æ ¼å¼
        for encoding in ['utf-8', 'gbk', 'gb2312']:
            try:
                data = pd.read_csv(file_path, encoding=encoding)
                break
            except UnicodeDecodeError:
                continue
    return data
```

##### 1.2 æ•°æ®è´¨é‡æ£€æŸ¥
- **å¿…è¦åˆ—éªŒè¯**ï¼šç¡®ä¿CommentTextå’ŒSentimentåˆ—å­˜åœ¨
- **æ•°æ®å®Œæ•´æ€§æ£€æŸ¥**ï¼šç»Ÿè®¡ç¼ºå¤±å€¼æ¯”ä¾‹
- **æ ‡ç­¾åˆ†å¸ƒåˆ†æ**ï¼šæ£€æŸ¥ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜
- **æ–‡æœ¬é•¿åº¦ç»Ÿè®¡**ï¼šåˆ†ææ–‡æœ¬é•¿åº¦åˆ†å¸ƒï¼Œç¡®å®šåˆé€‚çš„max_length

##### 1.3 æ•°æ®ç»Ÿè®¡åˆ†æ
```python
# æ•°æ®ç»Ÿè®¡ç¤ºä¾‹
print(f"æ•°æ®æ€»é‡: {len(data)}")
print(f"æ ‡ç­¾åˆ†å¸ƒ: {data['Sentiment'].value_counts()}")
print(f"å¹³å‡æ–‡æœ¬é•¿åº¦: {data['CommentText'].str.len().mean():.2f}")
print(f"æœ€å¤§æ–‡æœ¬é•¿åº¦: {data['CommentText'].str.len().max()}")
```

#### æ­¥éª¤2ï¼šæ•°æ®é¢„å¤„ç†

##### 2.1 æ–‡æœ¬æ¸…æ´—è¯¦ç»†æµç¨‹
```python
def clean_text(text):
    # 1. ç§»é™¤URLé“¾æ¥
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
    
    # 2. ç§»é™¤HTMLæ ‡ç­¾
    text = re.sub(r'<.*?>', '', text)
    
    # 3. ç§»é™¤ç”¨æˆ·æåŠå’Œè¯é¢˜æ ‡ç­¾
    text = re.sub(r'@\w+|#\w+', '', text)
    
    # 4. å¤„ç†è¡¨æƒ…ç¬¦å·ï¼ˆä¿ç•™æˆ–ç§»é™¤ï¼‰
    text = re.sub(r'[ğŸ˜€-ğŸ™ğŸŒ€-ğŸ—¿ğŸš€-ğŸ›¿]', '', text)
    
    # 5. æ ‡å‡†åŒ–æ ‡ç‚¹ç¬¦å·
    text = re.sub(r'[ï¼ï¼Ÿã€‚ï¼Œï¼›ï¼š]', lambda m: {'ï¼':'!', 'ï¼Ÿ':'?', 'ã€‚':'.', 'ï¼Œ':',', 'ï¼›':';', 'ï¼š':':'}[m.group()], text)
    
    # 6. ç§»é™¤å¤šä½™ç©ºæ ¼
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text
```

##### 2.2 æ–‡æœ¬åˆå¹¶ç­–ç•¥
```python
# å¤šæ¨¡æ€æ–‡æœ¬åˆå¹¶
def combine_text(comment, title):
    # ä½¿ç”¨ç‰¹æ®Šåˆ†éš”ç¬¦[SEP]è¿æ¥è¯„è®ºå’Œæ ‡é¢˜
    if pd.notna(title) and title.strip():
        return f"{comment} [SEP] {title}"
    return comment
```

##### 2.3 æ ‡ç­¾æ ‡å‡†åŒ–å¤„ç†
```python
def normalize_labels(label):
    label_mapping = {
        'positive': 2, 'pos': 2, 'good': 2, 'æ­£é¢': 2, 'ç§¯æ': 2,
        'negative': 0, 'neg': 0, 'bad': 0, 'è´Ÿé¢': 0, 'æ¶ˆæ': 0,
        'neutral': 1, 'neu': 1, 'ok': 1, 'ä¸­æ€§': 1, 'ä¸€èˆ¬': 1
    }
    return label_mapping.get(str(label).lower(), None)
```

##### 2.4 æ•°æ®åˆ†å‰²ç­–ç•¥
```python
# åˆ†å±‚æŠ½æ ·ç¡®ä¿æ ‡ç­¾åˆ†å¸ƒä¸€è‡´
from sklearn.model_selection import train_test_split

train_data, val_data = train_test_split(
    processed_data,
    test_size=0.2,
    random_state=42,
    stratify=processed_data['Sentiment']  # åˆ†å±‚æŠ½æ ·
)
```

##### 2.5 æ•°æ®å¢å¼ºæŠ€æœ¯
```python
class TextAugmentation:
    def __init__(self):
        self.augment_ratio = 0.3
    
    def synonym_replacement(self, text, n=1):
        """åŒä¹‰è¯æ›¿æ¢"""
        words = text.split()
        if len(words) < 2:
            return text
        
        # éšæœºé€‰æ‹©è¯æ±‡è¿›è¡Œæ›¿æ¢
        for _ in range(n):
            idx = random.randint(0, len(words)-1)
            # è¿™é‡Œå¯ä»¥é›†æˆåŒä¹‰è¯è¯å…¸
            words[idx] = self.get_synonym(words[idx])
        
        return ' '.join(words)
    
    def random_deletion(self, text, p=0.1):
        """éšæœºåˆ é™¤"""
        words = text.split()
        if len(words) == 1:
            return text
        
        new_words = []
        for word in words:
            if random.random() > p:
                new_words.append(word)
        
        return ' '.join(new_words) if new_words else text
```

#### æ­¥éª¤3ï¼šæ¨¡å‹æ„å»º

##### 3.1 BERTæ¨¡å‹æ¶æ„è®¾è®¡
```python
class BertSentimentClassifier(nn.Module):
    def __init__(self, config, pretrained_model_name='bert-base-chinese'):
        super().__init__()
        self.config = config
        
        # åŠ è½½é¢„è®­ç»ƒBERTæ¨¡å‹
        self.bert = BertModel.from_pretrained(pretrained_model_name)
        
        # åˆ†ç±»å¤´è®¾è®¡
        self.dropout = nn.Dropout(config.DROPOUT_RATE)
        self.classifier = nn.Sequential(
            nn.Linear(self.bert.config.hidden_size, 256),
            nn.ReLU(),
            nn.Dropout(config.DROPOUT_RATE),
            nn.Linear(256, config.NUM_CLASSES)
        )
        
        # åˆå§‹åŒ–åˆ†ç±»å±‚æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–æ–°å¢å±‚çš„æƒé‡"""
        for module in self.classifier:
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                nn.init.zeros_(module.bias)
```

##### 3.2 å±‚å†»ç»“ç­–ç•¥
```python
def freeze_bert_layers(self, num_layers_to_freeze):
    """å†»ç»“BERTçš„å‰å‡ å±‚"""
    # å†»ç»“embeddingå±‚
    for param in self.bert.embeddings.parameters():
        param.requires_grad = False
    
    # å†»ç»“æŒ‡å®šæ•°é‡çš„encoderå±‚
    for i in range(num_layers_to_freeze):
        for param in self.bert.encoder.layer[i].parameters():
            param.requires_grad = False
    
    print(f"å·²å†»ç»“BERTçš„å‰{num_layers_to_freeze}å±‚")
```

#### æ­¥éª¤4ï¼šè®­ç»ƒå¾ªç¯è¯¦ç»†å®ç°

##### 4.1 å‰å‘ä¼ æ’­è¿‡ç¨‹
```python
def forward(self, input_ids, attention_mask, labels=None):
    # BERTç¼–ç 
    outputs = self.bert(
        input_ids=input_ids,
        attention_mask=attention_mask,
        return_dict=True
    )
    
    # è·å–[CLS] tokençš„è¡¨ç¤º
    pooled_output = outputs.pooler_output
    
    # Dropoutæ­£åˆ™åŒ–
    pooled_output = self.dropout(pooled_output)
    
    # åˆ†ç±»é¢„æµ‹
    logits = self.classifier(pooled_output)
    
    # è®¡ç®—æŸå¤±
    loss = None
    if labels is not None:
        if self.config.USE_FOCAL_LOSS:
            loss_fn = FocalLoss(alpha=1, gamma=2)
        else:
            loss_fn = nn.CrossEntropyLoss()
        loss = loss_fn(logits, labels)
    
    return {'loss': loss, 'logits': logits}
```

##### 4.2 è®­ç»ƒå¾ªç¯æ§åˆ¶
```python
def train_epoch(self, model, train_loader, optimizer, scheduler):
    model.train()
    total_loss = 0
    progress_bar = tqdm(train_loader, desc='Training')
    
    for batch_idx, batch in enumerate(progress_bar):
        # æ•°æ®ç§»åŠ¨åˆ°GPU
        input_ids = batch['input_ids'].to(self.device)
        attention_mask = batch['attention_mask'].to(self.device)
        labels = batch['labels'].to(self.device)
        
        # æ¸…é›¶æ¢¯åº¦
        optimizer.zero_grad()
        
        # å‰å‘ä¼ æ’­
        outputs = model(input_ids, attention_mask, labels)
        loss = outputs['loss']
        
        # åå‘ä¼ æ’­
        loss.backward()
        
        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        # å‚æ•°æ›´æ–°
        optimizer.step()
        scheduler.step()
        
        # ç»Ÿè®¡æŸå¤±
        total_loss += loss.item()
        
        # æ›´æ–°è¿›åº¦æ¡
        progress_bar.set_postfix({
            'loss': f'{loss.item():.4f}',
            'avg_loss': f'{total_loss/(batch_idx+1):.4f}',
            'lr': f'{scheduler.get_last_lr()[0]:.2e}'
        })
    
    return total_loss / len(train_loader)
```

##### 4.3 å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥
```python
# é¢„çƒ­+ä½™å¼¦é€€ç«è°ƒåº¦
from transformers import get_cosine_schedule_with_warmup

scheduler = get_cosine_schedule_with_warmup(
    optimizer,
    num_warmup_steps=500,  # é¢„çƒ­æ­¥æ•°
    num_training_steps=total_steps,
    num_cycles=0.5  # ä½™å¼¦å‘¨æœŸæ•°
)
```

#### æ­¥éª¤5ï¼šæ¨¡å‹è¯„ä¼°

##### 5.1 éªŒè¯é›†è¯„ä¼°æµç¨‹
```python
def evaluate_model(self, model, val_loader):
    model.eval()
    all_predictions = []
    all_labels = []
    total_loss = 0
    
    with torch.no_grad():
        for batch in tqdm(val_loader, desc='Evaluating'):
            input_ids = batch['input_ids'].to(self.device)
            attention_mask = batch['attention_mask'].to(self.device)
            labels = batch['labels'].to(self.device)
            
            outputs = model(input_ids, attention_mask, labels)
            loss = outputs['loss']
            logits = outputs['logits']
            
            # è·å–é¢„æµ‹ç»“æœ
            predictions = torch.argmax(logits, dim=-1)
            
            all_predictions.extend(predictions.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            total_loss += loss.item()
    
    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    metrics = self.calculate_metrics(all_labels, all_predictions)
    metrics['avg_loss'] = total_loss / len(val_loader)
    
    return metrics
```

##### 5.2 è¯„ä¼°æŒ‡æ ‡è®¡ç®—
```python
def calculate_metrics(self, y_true, y_pred):
    from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report
    
    metrics = {
        'accuracy': accuracy_score(y_true, y_pred),
        'f1_macro': f1_score(y_true, y_pred, average='macro'),
        'f1_weighted': f1_score(y_true, y_pred, average='weighted'),
        'precision_macro': precision_score(y_true, y_pred, average='macro'),
        'recall_macro': recall_score(y_true, y_pred, average='macro')
    }
    
    # è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
    report = classification_report(y_true, y_pred, target_names=['è´Ÿé¢', 'ä¸­æ€§', 'æ­£é¢'])
    
    return metrics, report
```

##### 5.3 æ—©åœæœºåˆ¶å®ç°
```python
class EarlyStopping:
    def __init__(self, patience=3, min_delta=0.001):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_score = None
        self.early_stop = False
    
    def __call__(self, val_score):
        if self.best_score is None:
            self.best_score = val_score
        elif val_score < self.best_score + self.min_delta:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_score = val_score
            self.counter = 0
```

#### æ­¥éª¤6ï¼šæ¨¡å‹ä¿å­˜ä¸ç®¡ç†

##### 6.1 æ¨¡å‹ä¿å­˜ç­–ç•¥
```python
def save_model(self, model, optimizer, epoch, metrics, is_best=False):
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'metrics': metrics,
        'config': self.config.__dict__
    }
    
    # ä¿å­˜æ£€æŸ¥ç‚¹
    checkpoint_path = f"checkpoints/checkpoint_epoch_{epoch}.pth"
    torch.save(checkpoint, checkpoint_path)
    
    # ä¿å­˜æœ€ä½³æ¨¡å‹
    if is_best:
        best_model_path = f"models/best_model_epoch_{epoch}_f1_{metrics['f1_macro']:.4f}.pth"
        torch.save(checkpoint, best_model_path)
        print(f"æœ€ä½³æ¨¡å‹å·²ä¿å­˜: {best_model_path}")
```

##### 6.2 æ¨¡å‹åŠ è½½ä¸æ¢å¤
```python
def load_model(self, model_path, model, optimizer=None):
    checkpoint = torch.load(model_path, map_location=self.device)
    
    model.load_state_dict(checkpoint['model_state_dict'])
    
    if optimizer and 'optimizer_state_dict' in checkpoint:
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    
    epoch = checkpoint.get('epoch', 0)
    metrics = checkpoint.get('metrics', {})
    
    print(f"æ¨¡å‹å·²åŠ è½½ï¼Œepoch: {epoch}, F1: {metrics.get('f1_macro', 'N/A')}")
    
    return epoch, metrics
```

---

## 3. ä¸»è¦æŠ€æœ¯å®ç°åŸç†

### 3.1 BERTæ¨¡å‹æ·±åº¦è§£æ

#### 3.1.1 Transformeræ¶æ„æ ¸å¿ƒæœºåˆ¶

##### è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆSelf-Attentionï¼‰
```python
# è‡ªæ³¨æ„åŠ›è®¡ç®—å…¬å¼å®ç°
def scaled_dot_product_attention(Q, K, V, mask=None):
    """
    Q: QueryçŸ©é˜µ [batch_size, seq_len, d_k]
    K: KeyçŸ©é˜µ [batch_size, seq_len, d_k]
    V: ValueçŸ©é˜µ [batch_size, seq_len, d_v]
    """
    d_k = Q.size(-1)
    
    # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°: QK^T / sqrt(d_k)
    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)
    
    # åº”ç”¨maskï¼ˆå¦‚æœæœ‰ï¼‰
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)
    
    # Softmaxå½’ä¸€åŒ–
    attention_weights = F.softmax(scores, dim=-1)
    
    # åŠ æƒæ±‚å’Œ
    output = torch.matmul(attention_weights, V)
    
    return output, attention_weights
```

**è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„ä¼˜åŠ¿ï¼š**
- **å¹¶è¡Œè®¡ç®—**ï¼šä¸åŒä½ç½®å¯ä»¥åŒæ—¶è®¡ç®—ï¼Œæé«˜æ•ˆç‡
- **é•¿è·ç¦»ä¾èµ–**ï¼šç›´æ¥å»ºæ¨¡ä»»æ„ä¸¤ä¸ªä½ç½®ä¹‹é—´çš„å…³ç³»
- **åŠ¨æ€æƒé‡**ï¼šæ ¹æ®è¾“å…¥å†…å®¹åŠ¨æ€è°ƒæ•´æ³¨æ„åŠ›æƒé‡

##### å¤šå¤´æ³¨æ„åŠ›ï¼ˆMulti-Head Attentionï¼‰
```python
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        
        # çº¿æ€§å˜æ¢å±‚
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
    
    def forward(self, query, key, value, mask=None):
        batch_size = query.size(0)
        
        # 1. çº¿æ€§å˜æ¢å¹¶é‡å¡‘ä¸ºå¤šå¤´
        Q = self.W_q(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        K = self.W_k(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        V = self.W_v(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        
        # 2. åº”ç”¨æ³¨æ„åŠ›æœºåˆ¶
        attention_output, attention_weights = scaled_dot_product_attention(Q, K, V, mask)
        
        # 3. è¿æ¥å¤šå¤´å¹¶é€šè¿‡è¾“å‡ºæŠ•å½±
        attention_output = attention_output.transpose(1, 2).contiguous().view(
            batch_size, -1, self.d_model
        )
        
        return self.W_o(attention_output)
```

##### ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰
```python
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_seq_length=512):
        super().__init__()
        
        pe = torch.zeros(max_seq_length, d_model)
        position = torch.arange(0, max_seq_length).unsqueeze(1).float()
        
        # è®¡ç®—ä½ç½®ç¼–ç 
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           -(math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)  # å¶æ•°ä½ç½®
        pe[:, 1::2] = torch.cos(position * div_term)  # å¥‡æ•°ä½ç½®
        
        self.register_buffer('pe', pe.unsqueeze(0))
    
    def forward(self, x):
        return x + self.pe[:, :x.size(1)]
```

#### 3.1.2 BERTç‰¹æœ‰æœºåˆ¶è¯¦è§£

##### åŒå‘ç¼–ç å™¨æ¶æ„
```python
class BertEncoder(nn.Module):
    def __init__(self, config):
        super().__init__()
        # å¤šå±‚Transformerç¼–ç å™¨
        self.layer = nn.ModuleList([
            BertLayer(config) for _ in range(config.num_hidden_layers)
        ])
    
    def forward(self, hidden_states, attention_mask=None):
        all_hidden_states = []
        all_attentions = []
        
        for layer_module in self.layer:
            hidden_states, attention_probs = layer_module(
                hidden_states, attention_mask
            )
            all_hidden_states.append(hidden_states)
            all_attentions.append(attention_probs)
        
        return {
            'last_hidden_state': hidden_states,
            'hidden_states': all_hidden_states,
            'attentions': all_attentions
        }
```

##### é¢„è®­ç»ƒä»»åŠ¡å®ç°

**1. Masked Language Model (MLM)**
```python
class BertForMaskedLM(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.bert = BertModel(config)
        self.cls = BertLMPredictionHead(config)
    
    def forward(self, input_ids, attention_mask=None, labels=None):
        outputs = self.bert(input_ids, attention_mask)
        sequence_output = outputs.last_hidden_state
        
        # é¢„æµ‹è¢«maskçš„token
        prediction_scores = self.cls(sequence_output)
        
        masked_lm_loss = None
        if labels is not None:
            loss_fct = nn.CrossEntropyLoss()  # -100 index = padding token
            masked_lm_loss = loss_fct(
                prediction_scores.view(-1, self.config.vocab_size), 
                labels.view(-1)
            )
        
        return {
            'loss': masked_lm_loss,
            'logits': prediction_scores
        }
```

**2. Next Sentence Prediction (NSP)**
```python
class BertForNextSentencePrediction(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.bert = BertModel(config)
        self.cls = BertOnlyNSPHead(config)
    
    def forward(self, input_ids, attention_mask=None, next_sentence_label=None):
        outputs = self.bert(input_ids, attention_mask)
        pooled_output = outputs.pooler_output
        
        seq_relationship_scores = self.cls(pooled_output)
        
        next_sentence_loss = None
        if next_sentence_label is not None:
            loss_fct = nn.CrossEntropyLoss()
            next_sentence_loss = loss_fct(
                seq_relationship_scores.view(-1, 2), 
                next_sentence_label.view(-1)
            )
        
        return {
            'loss': next_sentence_loss,
            'logits': seq_relationship_scores
        }
```

### 3.2 æƒ…æ„Ÿåˆ†ææ·±åº¦å®ç°

#### 3.2.1 æ–‡æœ¬è¡¨ç¤ºä¸ç¼–ç 

##### BERT Tokenizationè¯¦ç»†è¿‡ç¨‹
```python
class BertTokenizationPipeline:
    def __init__(self, model_name='bert-base-chinese'):
        self.tokenizer = BertTokenizer.from_pretrained(model_name)
        self.max_length = 512
    
    def encode_text(self, text, title=None):
        # 1. æ–‡æœ¬é¢„å¤„ç†
        text = self.preprocess_text(text)
        
        # 2. æ„å»ºè¾“å…¥åºåˆ—
        if title:
            # å¤šæ¨¡æ€è¾“å…¥ï¼š[CLS] comment [SEP] title [SEP]
            encoded = self.tokenizer.encode_plus(
                text, title,
                add_special_tokens=True,
                max_length=self.max_length,
                padding='max_length',
                truncation=True,
                return_attention_mask=True,
                return_tensors='pt'
            )
        else:
            # å•æ¨¡æ€è¾“å…¥ï¼š[CLS] comment [SEP]
            encoded = self.tokenizer.encode_plus(
                text,
                add_special_tokens=True,
                max_length=self.max_length,
                padding='max_length',
                truncation=True,
                return_attention_mask=True,
                return_tensors='pt'
            )
        
        return {
            'input_ids': encoded['input_ids'],
            'attention_mask': encoded['attention_mask'],
            'token_type_ids': encoded.get('token_type_ids', None)
        }
    
    def decode_tokens(self, input_ids):
        """å°†token IDè½¬æ¢å›æ–‡æœ¬ï¼ˆç”¨äºè°ƒè¯•ï¼‰"""
        return self.tokenizer.decode(input_ids, skip_special_tokens=False)
```

##### ç‰¹å¾æå–ç­–ç•¥
```python
class BertFeatureExtractor:
    def __init__(self, model):
        self.model = model
    
    def extract_features(self, input_ids, attention_mask, layer_indices=[-4, -3, -2, -1]):
        """æå–æŒ‡å®šå±‚çš„ç‰¹å¾"""
        with torch.no_grad():
            outputs = self.model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                output_hidden_states=True,
                output_attentions=True
            )
        
        # è·å–æŒ‡å®šå±‚çš„éšè—çŠ¶æ€
        hidden_states = outputs.hidden_states
        selected_layers = [hidden_states[i] for i in layer_indices]
        
        # ç‰¹å¾èåˆç­–ç•¥
        features = {
            'cls_token': selected_layers[-1][:, 0, :],  # [CLS] token
            'mean_pooling': torch.mean(selected_layers[-1], dim=1),  # å¹³å‡æ± åŒ–
            'max_pooling': torch.max(selected_layers[-1], dim=1)[0],  # æœ€å¤§æ± åŒ–
            'concat_layers': torch.cat(selected_layers, dim=-1)  # å±‚æ‹¼æ¥
        }
        
        return features
```

#### 3.2.2 é«˜çº§åˆ†ç±»å™¨è®¾è®¡

##### å¤šå±‚æ„ŸçŸ¥æœºåˆ†ç±»å™¨
```python
class AdvancedSentimentClassifier(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        
        # BERTç¼–ç å™¨
        self.bert = BertModel.from_pretrained(config.MODEL_NAME)
        
        # ç‰¹å¾èåˆå±‚
        self.feature_fusion = nn.ModuleDict({
            'cls_projection': nn.Linear(768, 256),
            'mean_projection': nn.Linear(768, 256),
            'max_projection': nn.Linear(768, 256)
        })
        
        # æ³¨æ„åŠ›èåˆæœºåˆ¶
        self.attention_fusion = nn.MultiheadAttention(
            embed_dim=256, num_heads=8, batch_first=True
        )
        
        # åˆ†ç±»å¤´
        self.classifier = nn.Sequential(
            nn.Linear(768, 512),
            nn.LayerNorm(512),
            nn.ReLU(),
            nn.Dropout(config.DROPOUT_RATE),
            
            nn.Linear(512, 256),
            nn.LayerNorm(256),
            nn.ReLU(),
            nn.Dropout(config.DROPOUT_RATE),
            
            nn.Linear(256, config.NUM_CLASSES)
        )
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """Xavieråˆå§‹åŒ–"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
            elif isinstance(module, nn.LayerNorm):
                nn.init.ones_(module.weight)
                nn.init.zeros_(module.bias)
    
    def forward(self, input_ids, attention_mask, labels=None):
        # BERTç¼–ç 
        bert_outputs = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask,
            output_hidden_states=True
        )
        
        # å¤šç§ç‰¹å¾æå–
        last_hidden_state = bert_outputs.last_hidden_state
        pooler_output = bert_outputs.pooler_output
        
        # ç‰¹å¾èåˆ
        cls_features = self.feature_fusion['cls_projection'](pooler_output)
        mean_features = self.feature_fusion['mean_projection'](
            torch.mean(last_hidden_state, dim=1)
        )
        max_features = self.feature_fusion['max_projection'](
            torch.max(last_hidden_state, dim=1)[0]
        )
        
        # æ‹¼æ¥ç‰¹å¾
        fused_features = torch.cat([cls_features, mean_features, max_features], dim=-1)
        
        # åˆ†ç±»é¢„æµ‹
        logits = self.classifier(fused_features)
        
        # è®¡ç®—æŸå¤±
        loss = None
        if labels is not None:
            if self.config.USE_FOCAL_LOSS:
                loss_fn = FocalLoss(alpha=self.config.FOCAL_ALPHA, gamma=self.config.FOCAL_GAMMA)
            else:
                loss_fn = nn.CrossEntropyLoss(weight=self.config.CLASS_WEIGHTS)
            loss = loss_fn(logits, labels)
        
        return {
            'loss': loss,
            'logits': logits,
            'features': fused_features
        }
```

### 3.3 è®­ç»ƒä¼˜åŒ–æŠ€æœ¯æ·±åº¦è§£æ

#### 3.3.1 é«˜çº§æŸå¤±å‡½æ•°

##### Focal Losså®ç°
```python
class FocalLoss(nn.Module):
    """Focal Loss for addressing class imbalance"""
    def __init__(self, alpha=1, gamma=2, reduction='mean'):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
    
    def forward(self, inputs, targets):
        ce_loss = F.cross_entropy(inputs, targets, reduction='none')
        pt = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss
```

##### Label Smoothingå®ç°
```python
class LabelSmoothingCrossEntropy(nn.Module):
    def __init__(self, smoothing=0.1):
        super().__init__()
        self.smoothing = smoothing
    
    def forward(self, pred, target):
        n_class = pred.size(1)
        one_hot = torch.zeros_like(pred).scatter(1, target.unsqueeze(1), 1)
        smooth_one_hot = one_hot * (1 - self.smoothing) + self.smoothing / n_class
        
        log_prob = F.log_softmax(pred, dim=1)
        loss = (-smooth_one_hot * log_prob).sum(dim=1).mean()
        
        return loss
```

#### 3.3.2 é«˜çº§ä¼˜åŒ–ç­–ç•¥

##### è‡ªé€‚åº”å­¦ä¹ ç‡è°ƒåº¦
```python
class CosineAnnealingWarmupRestarts(torch.optim.lr_scheduler._LRScheduler):
    def __init__(self, optimizer, first_cycle_steps, cycle_mult=1., max_lr=0.1, 
                 min_lr=0.001, warmup_steps=0, gamma=1.):
        self.first_cycle_steps = first_cycle_steps
        self.cycle_mult = cycle_mult
        self.base_max_lr = max_lr
        self.max_lr = max_lr
        self.min_lr = min_lr
        self.warmup_steps = warmup_steps
        self.gamma = gamma
        
        self.cur_cycle_steps = first_cycle_steps
        self.cycle = 0
        self.step_in_cycle = 0
        
        super().__init__(optimizer)
        
        # åˆå§‹åŒ–å­¦ä¹ ç‡
        self.init_lr()
    
    def init_lr(self):
        self.base_lrs = []
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = self.min_lr
            self.base_lrs.append(self.min_lr)
    
    def get_lr(self):
        if self.step_in_cycle == -1:
            return self.base_lrs
        elif self.step_in_cycle < self.warmup_steps:
            return [(self.max_lr - base_lr) * self.step_in_cycle / self.warmup_steps + base_lr
                    for base_lr in self.base_lrs]
        else:
            return [base_lr + (self.max_lr - base_lr) * 
                    (1 + math.cos(math.pi * (self.step_in_cycle - self.warmup_steps) / 
                                  (self.cur_cycle_steps - self.warmup_steps))) / 2
                    for base_lr in self.base_lrs]
```

##### æ¢¯åº¦ç´¯ç§¯ä¸æ··åˆç²¾åº¦è®­ç»ƒ
```python
class AdvancedTrainer:
    def __init__(self, config):
        self.config = config
        self.scaler = GradScaler() if config.USE_AMP else None
        self.gradient_accumulation_steps = config.GRADIENT_ACCUMULATION_STEPS
    
    def train_step(self, model, batch, optimizer, step):
        model.train()
        
        # æ··åˆç²¾åº¦è®­ç»ƒ
        if self.config.USE_AMP:
            with autocast():
                outputs = model(**batch)
                loss = outputs['loss'] / self.gradient_accumulation_steps
            
            # åå‘ä¼ æ’­
            self.scaler.scale(loss).backward()
            
            # æ¢¯åº¦ç´¯ç§¯
            if (step + 1) % self.gradient_accumulation_steps == 0:
                # æ¢¯åº¦è£å‰ª
                self.scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                
                # å‚æ•°æ›´æ–°
                self.scaler.step(optimizer)
                self.scaler.update()
                optimizer.zero_grad()
        else:
            outputs = model(**batch)
            loss = outputs['loss'] / self.gradient_accumulation_steps
            
            loss.backward()
            
            if (step + 1) % self.gradient_accumulation_steps == 0:
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()
                optimizer.zero_grad()
        
        return loss.item() * self.gradient_accumulation_steps
```

### 3.4 æ•°æ®å¤„ç†æŠ€æœ¯æ·±åº¦å®ç°

#### 3.4.1 é«˜çº§æ–‡æœ¬é¢„å¤„ç†

##### æ™ºèƒ½æ–‡æœ¬æ¸…æ´—
```python
class AdvancedTextPreprocessor:
    def __init__(self):
        self.emoji_pattern = re.compile(
            "["
            "\U0001F600-\U0001F64F"  # emoticons
            "\U0001F300-\U0001F5FF"  # symbols & pictographs
            "\U0001F680-\U0001F6FF"  # transport & map symbols
            "\U0001F1E0-\U0001F1FF"  # flags (iOS)
            "\U00002702-\U000027B0"
            "\U000024C2-\U0001F251"
            "]+", flags=re.UNICODE
        )
        
        # åŠ è½½åœç”¨è¯
        self.stopwords = self.load_stopwords()
        
        # åŠ è½½åŒä¹‰è¯è¯å…¸
        self.synonym_dict = self.load_synonyms()
    
    def clean_text(self, text, preserve_emoji=False):
        """é«˜çº§æ–‡æœ¬æ¸…æ´—"""
        if not isinstance(text, str):
            return ""
        
        # 1. æ ‡å‡†åŒ–Unicode
        text = unicodedata.normalize('NFKC', text)
        
        # 2. ç§»é™¤æˆ–ä¿ç•™è¡¨æƒ…ç¬¦å·
        if not preserve_emoji:
            text = self.emoji_pattern.sub('', text)
        
        # 3. å¤„ç†URLå’Œé‚®ç®±
        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '[URL]', text)
        text = re.sub(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', '[EMAIL]', text)
        
        # 4. å¤„ç†@ç”¨æˆ·å’Œ#è¯é¢˜
        text = re.sub(r'@[A-Za-z0-9_]+', '[USER]', text)
        text = re.sub(r'#[A-Za-z0-9_]+', '[HASHTAG]', text)
        
        # 5. æ ‡å‡†åŒ–æ ‡ç‚¹ç¬¦å·
        punctuation_map = {
            'ï¼': '!', 'ï¼Ÿ': '?', 'ã€‚': '.', 'ï¼Œ': ',', 
            'ï¼›': ';', 'ï¼š': ':', 'ï¼ˆ': '(', 'ï¼‰': ')',
            'ã€': '[', 'ã€‘': ']', 'ã€Œ': '"', 'ã€': '"'
        }
        for zh_punct, en_punct in punctuation_map.items():
            text = text.replace(zh_punct, en_punct)
        
        # 6. ç§»é™¤å¤šä½™ç©ºæ ¼å’Œæ¢è¡Œ
        text = re.sub(r'\s+', ' ', text)
        text = text.strip()
        
        return text
    
    def segment_text(self, text):
        """ä¸­æ–‡åˆ†è¯"""
        import jieba
        
        # åˆ†è¯
        words = list(jieba.cut(text))
        
        # è¿‡æ»¤åœç”¨è¯
        words = [word for word in words if word not in self.stopwords and len(word.strip()) > 0]
        
        return words
```

#### 3.4.2 é«˜çº§æ•°æ®å¢å¼º

##### åŸºäºè¯­ä¹‰çš„æ•°æ®å¢å¼º
```python
class SemanticDataAugmentation:
    def __init__(self):
        # åŠ è½½é¢„è®­ç»ƒè¯å‘é‡
        self.word_vectors = self.load_word_vectors()
        
        # åŠ è½½åŒä¹‰è¯è¯å…¸
        self.synonym_dict = self.load_synonym_dict()
    
    def synonym_replacement(self, text, n=1, threshold=0.8):
        """åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦çš„åŒä¹‰è¯æ›¿æ¢"""
        words = text.split()
        if len(words) < 2:
            return text
        
        new_words = words.copy()
        random_word_list = list(set([word for word in words if word.isalpha()]))
        random.shuffle(random_word_list)
        
        num_replaced = 0
        for random_word in random_word_list:
            if num_replaced >= n:
                break
            
            synonyms = self.get_synonyms(random_word, threshold)
            if synonyms:
                synonym = random.choice(synonyms)
                new_words = [synonym if word == random_word else word for word in new_words]
                num_replaced += 1
        
        return ' '.join(new_words)
    
    def back_translation(self, text, intermediate_lang='en'):
        """å›è¯‘æ•°æ®å¢å¼º"""
        # è¿™é‡Œå¯ä»¥é›†æˆç¿»è¯‘API
        # ä¸­æ–‡ -> è‹±æ–‡ -> ä¸­æ–‡
        try:
            # æ¨¡æ‹Ÿç¿»è¯‘è¿‡ç¨‹
            translated = self.translate(text, 'zh', intermediate_lang)
            back_translated = self.translate(translated, intermediate_lang, 'zh')
            return back_translated
        except:
            return text
    
    def contextual_word_embedding_replacement(self, text, model_name='bert-base-chinese'):
        """åŸºäºä¸Šä¸‹æ–‡è¯åµŒå…¥çš„æ›¿æ¢"""
        from transformers import BertTokenizer, BertForMaskedLM
        
        tokenizer = BertTokenizer.from_pretrained(model_name)
        model = BertForMaskedLM.from_pretrained(model_name)
        
        words = text.split()
        if len(words) < 3:
            return text
        
        # éšæœºé€‰æ‹©ä¸€ä¸ªè¯è¿›è¡Œæ›¿æ¢
        word_idx = random.randint(1, len(words) - 2)
        
        # æ„å»ºmaskedè¾“å…¥
        masked_text = ' '.join(words[:word_idx] + ['[MASK]'] + words[word_idx+1:])
        
        # è·å–é¢„æµ‹
        inputs = tokenizer(masked_text, return_tensors='pt')
        with torch.no_grad():
            outputs = model(**inputs)
        
        # è·å–top-ké¢„æµ‹
        mask_token_index = torch.where(inputs['input_ids'] == tokenizer.mask_token_id)[1]
        mask_token_logits = outputs.logits[0, mask_token_index, :]
        top_k_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()
        
        # é€‰æ‹©ä¸€ä¸ªåˆé€‚çš„æ›¿æ¢è¯
        for token_id in top_k_tokens:
            candidate = tokenizer.decode([token_id])
            if candidate != words[word_idx] and candidate.isalpha():
                words[word_idx] = candidate
                break
        
        return ' '.join(words)
```

---

## 4. å…¶ä»–é‡è¦é—®é¢˜

### 4.1 æ¨¡å‹æ€§èƒ½ä¼˜åŒ–

#### 4.1.1 è¶…å‚æ•°è°ƒä¼˜
- **å­¦ä¹ ç‡**ï¼šé€šå¸¸åœ¨1e-5åˆ°5e-5ä¹‹é—´
- **æ‰¹æ¬¡å¤§å°**ï¼šæ ¹æ®æ˜¾å­˜é™åˆ¶é€‰æ‹©8-32
- **è®­ç»ƒè½®æ•°**ï¼šé€šå¸¸3-5è½®ï¼Œä½¿ç”¨æ—©åœé˜²æ­¢è¿‡æ‹Ÿåˆ
- **æœ€å¤§åºåˆ—é•¿åº¦**ï¼šå¹³è¡¡æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡

#### 4.1.2 æ¨¡å‹é›†æˆ
- **å¤šæ¨¡å‹æŠ•ç¥¨**ï¼šè®­ç»ƒå¤šä¸ªæ¨¡å‹è¿›è¡Œé›†æˆé¢„æµ‹
- **äº¤å‰éªŒè¯**ï¼šä½¿ç”¨KæŠ˜äº¤å‰éªŒè¯æé«˜æ¨¡å‹ç¨³å®šæ€§

### 4.2 å·¥ç¨‹å®è·µé—®é¢˜

#### 4.2.1 å†…å­˜å’Œè®¡ç®—ä¼˜åŒ–
- **æ··åˆç²¾åº¦è®­ç»ƒ**ï¼šä½¿ç”¨FP16å‡å°‘æ˜¾å­˜å ç”¨
- **æ¢¯åº¦ç´¯ç§¯**ï¼šåœ¨å°æ‰¹æ¬¡ä¸Šç´¯ç§¯æ¢¯åº¦
- **æ¨¡å‹å¹¶è¡Œ**ï¼šåœ¨å¤šGPUä¸Šåˆ†å¸ƒå¼è®­ç»ƒ

#### 4.2.2 æ•°æ®å¤„ç†æŒ‘æˆ˜
- **ç¼–ç é—®é¢˜**ï¼šå¤„ç†ä¸åŒç¼–ç æ ¼å¼çš„æ–‡æœ¬æ–‡ä»¶
- **æ•°æ®ä¸å¹³è¡¡**ï¼šä½¿ç”¨é‡é‡‡æ ·æˆ–åŠ æƒæŸå¤±å‡½æ•°
- **å™ªå£°æ•°æ®**ï¼šè®¾è®¡é²æ£’çš„æ•°æ®æ¸…æ´—ç­–ç•¥

### 4.3 è¯„ä¼°æŒ‡æ ‡è§£é‡Š

#### 4.3.1 ä¸»è¦æŒ‡æ ‡
- **å‡†ç¡®ç‡ï¼ˆAccuracyï¼‰**ï¼šæ­£ç¡®é¢„æµ‹çš„æ ·æœ¬æ¯”ä¾‹
- **ç²¾ç¡®ç‡ï¼ˆPrecisionï¼‰**ï¼šé¢„æµ‹ä¸ºæ­£ç±»ä¸­å®é™…ä¸ºæ­£ç±»çš„æ¯”ä¾‹
- **å¬å›ç‡ï¼ˆRecallï¼‰**ï¼šå®é™…æ­£ç±»ä¸­è¢«æ­£ç¡®é¢„æµ‹çš„æ¯”ä¾‹
- **F1åˆ†æ•°**ï¼šç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡

#### 4.3.2 å¤šåˆ†ç±»è¯„ä¼°
- **å®å¹³å‡**ï¼šå„ç±»åˆ«æŒ‡æ ‡çš„ç®€å•å¹³å‡
- **å¾®å¹³å‡**ï¼šå…¨å±€è®¡ç®—æŒ‡æ ‡
- **åŠ æƒå¹³å‡**ï¼šæŒ‰ç±»åˆ«æ ·æœ¬æ•°åŠ æƒå¹³å‡

### 4.4 éƒ¨ç½²å’Œåº”ç”¨

#### 4.4.1 æ¨¡å‹éƒ¨ç½²
- **æ¨¡å‹åºåˆ—åŒ–**ï¼šä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹å‚æ•°
- **æ¨ç†ä¼˜åŒ–**ï¼šä½¿ç”¨ONNXæˆ–TensorRTåŠ é€Ÿæ¨ç†
- **APIæœåŠ¡**ï¼šæ„å»ºRESTful APIæä¾›é¢„æµ‹æœåŠ¡

#### 4.4.2 å®æ—¶é¢„æµ‹
- **æ‰¹é‡é¢„æµ‹**ï¼šå¤„ç†å¤§é‡æ–‡æœ¬çš„æ‰¹é‡é¢„æµ‹
- **æµå¼å¤„ç†**ï¼šå®æ—¶å¤„ç†æ–°çš„è¯„è®ºæ•°æ®
- **ç¼“å­˜æœºåˆ¶**ï¼šç¼“å­˜å¸¸è§æ–‡æœ¬çš„é¢„æµ‹ç»“æœ

---

## 5. é¡¹ç›®ä¼˜åŠ¿ä¸ç‰¹è‰²

### 5.1 æŠ€æœ¯ä¼˜åŠ¿
1. **å…ˆè¿›çš„æ¨¡å‹æ¶æ„**ï¼šä½¿ç”¨BERTç­‰é¢„è®­ç»ƒæ¨¡å‹
2. **å®Œæ•´çš„å·¥ç¨‹å®ç°**ï¼šä»æ•°æ®å¤„ç†åˆ°æ¨¡å‹éƒ¨ç½²çš„å®Œæ•´æµç¨‹
3. **çµæ´»çš„é…ç½®ç³»ç»Ÿ**ï¼šæ”¯æŒå¤šç§å‚æ•°é…ç½®å’Œå®éªŒè®¾ç½®
4. **é²æ£’çš„æ•°æ®å¤„ç†**ï¼šå¤„ç†å¤šç§æ•°æ®æ ¼å¼å’Œç¼–ç é—®é¢˜

### 5.2 å®ç”¨ç‰¹è‰²
1. **ç°åœºè€ƒæ ¸é€‚é…**ï¼šä¸“é—¨çš„æ•°æ®åˆ’åˆ†å’Œè®­ç»ƒå·¥å…·
2. **è¯¦ç»†çš„ç›‘æ§**ï¼šè®­ç»ƒè¿‡ç¨‹çš„å¯è§†åŒ–å’Œæ—¥å¿—è®°å½•
3. **å¤šç§è®­ç»ƒç­–ç•¥**ï¼šæ”¯æŒä¸åŒçš„ä¼˜åŒ–å’Œæ­£åˆ™åŒ–æŠ€æœ¯
4. **æ˜“äºä½¿ç”¨**ï¼šæä¾›è¯¦ç»†çš„ä½¿ç”¨æŒ‡å—å’Œç¤ºä¾‹

---

## 6. å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

### 6.1 æŠ€æœ¯é—®é¢˜
- **æ˜¾å­˜ä¸è¶³**ï¼šå‡å°æ‰¹æ¬¡å¤§å°ã€ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
- **è®­ç»ƒè¿‡æ…¢**ï¼šä½¿ç”¨æ··åˆç²¾åº¦ã€å¢å¤§æ‰¹æ¬¡å¤§å°
- **è¿‡æ‹Ÿåˆ**ï¼šå¢åŠ æ­£åˆ™åŒ–ã€ä½¿ç”¨æ—©åœã€æ•°æ®å¢å¼º
- **æ¬ æ‹Ÿåˆ**ï¼šå¢åŠ æ¨¡å‹å¤æ‚åº¦ã€è°ƒæ•´å­¦ä¹ ç‡

### 6.2 æ•°æ®é—®é¢˜
- **æ•°æ®ä¸å¹³è¡¡**ï¼šä½¿ç”¨é‡é‡‡æ ·ã€Focal Loss
- **æ•°æ®è´¨é‡å·®**ï¼šæ”¹è¿›æ•°æ®æ¸…æ´—ç­–ç•¥
- **æ ‡æ³¨ä¸ä¸€è‡´**ï¼šæ•°æ®æ ‡å‡†åŒ–ã€å¤šäººæ ‡æ³¨

### 6.3 å·¥ç¨‹é—®é¢˜
- **ç¯å¢ƒé…ç½®**ï¼šä½¿ç”¨requirements.txtç®¡ç†ä¾èµ–
- **ç‰ˆæœ¬å…¼å®¹**ï¼šå›ºå®šå…³é”®åº“çš„ç‰ˆæœ¬
- **è·¯å¾„é—®é¢˜**ï¼šä½¿ç”¨ç›¸å¯¹è·¯å¾„å’Œé…ç½®æ–‡ä»¶

---

## æ€»ç»“

æœ¬é¡¹ç›®æ˜¯ä¸€ä¸ªå®Œæ•´çš„æ·±åº¦å­¦ä¹ æƒ…æ„Ÿåˆ†æç³»ç»Ÿï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š

1. **æŠ€æœ¯å…ˆè¿›æ€§**ï¼šé‡‡ç”¨BERTç­‰æœ€æ–°çš„é¢„è®­ç»ƒæ¨¡å‹
2. **å·¥ç¨‹å®Œæ•´æ€§**ï¼šæ¶µç›–æ•°æ®å¤„ç†ã€æ¨¡å‹è®­ç»ƒã€è¯„ä¼°éƒ¨ç½²çš„å®Œæ•´æµç¨‹
3. **å®ç”¨æ€§å¼º**ï¼šé’ˆå¯¹ç°åœºè€ƒæ ¸ç­‰å®é™…åº”ç”¨åœºæ™¯è¿›è¡Œä¼˜åŒ–
4. **å¯æ‰©å±•æ€§å¥½**ï¼šæ¨¡å—åŒ–è®¾è®¡ï¼Œæ˜“äºæ‰©å±•å’Œä¿®æ”¹
5. **æ–‡æ¡£å®Œå–„**ï¼šæä¾›è¯¦ç»†çš„æŠ€æœ¯æ–‡æ¡£å’Œä½¿ç”¨æŒ‡å—

è¯¥é¡¹ç›®ä¸ä»…å±•ç¤ºäº†æ·±åº¦å­¦ä¹ åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„åº”ç”¨ï¼Œä¹Ÿä½“ç°äº†å®Œæ•´çš„æœºå™¨å­¦ä¹ å·¥ç¨‹å®è·µèƒ½åŠ›ã€‚